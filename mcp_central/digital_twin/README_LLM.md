# 基于大模型的数字分身 MCP

这是数字分身项目的升级版本，集成了大语言模型（LLM）和检索增强生成（RAG）技术，提供更智能、更个性化的AI助手服务。

## 🚀 核心特性

### 🧠 大模型驱动
- **深度理解**: 使用LLM分析聊天记录，提取深层次的用户特征
- **智能画像**: 自动生成结构化的用户画像，包含性格、兴趣、价值观等
- **自然对话**: 基于用户特征生成自然、贴心的个性化回答

### 📚 RAG检索增强
- **向量存储**: 将聊天记录转换为向量，支持语义相似度检索
- **记忆召回**: 根据当前问题检索相关的历史对话片段
- **上下文融合**: 结合检索到的记忆和大模型知识生成回答

### 🎯 个性化Prompt工程
- **动态构建**: 根据用户画像动态生成个性化的系统提示词
- **风格适配**: 让AI助手模拟用户的思维模式和表达习惯
- **情境感知**: 结合对话历史和相关记忆提供情境化回答

## 🏗️ 系统架构

```
📱 微信聊天数据
    ↓
📊 数据预处理 → 🤖 大语言模型 (Ollama/Qwen)
    ↓                    ↓
💾 向量数据库 ← 🧠 用户画像生成
(FAISS)              ↓
    ↓           💭 个性化Prompt构建
📚 RAG检索 →        ↓
    ↓           🎯 个性化回答生成
❓ 用户问题 →        ↓
              💬 智能助手服务
```

## 📦 环境要求

### 系统依赖
```bash
# Python 3.8+
pip install -r requirements_llm.txt
```

### 大模型服务
```bash
# 安装Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 启动服务
ollama serve

# 下载模型
ollama pull qwen2.5:7b
```

## 🛠️ 安装配置

### 1. 安装依赖
```bash
cd mcp_central/digital_twin
pip install -r requirements_llm.txt
```

### 2. 配置大模型
编辑 `config_llm.json`:
```json
{
  "llm_config": {
    "model_name": "qwen2.5:7b",
    "api_base": "http://localhost:11434",
    "max_tokens": 2048,
    "temperature": 0.7
  }
}
```

### 3. MCP服务器配置
```json
{
  "mcpServers": {
    "digital_twin_llm": {
      "command": "/path/to/fastmcp",
      "args": ["run", "/path/to/server_llm.py"]
    }
  }
}
```

## 🧪 快速测试

### 运行完整测试
```bash
python3 test_llm_standalone.py
```

### 测试输出示例
```
🤖 基于大模型的数字分身测试
============================================================

✅ Ollama服务运行正常，可用模型: ['qwen2.5:7b']
✅ 用户创建成功，ID: 1
✅ 成功导入 20 条消息到向量数据库
🔄 正在调用LLM进行深度分析，请稍候...
✅ 用户画像分析完成

📊 分析结果:
{
  "interests": ["人工智能", "深度学习", "跑步健身", "阅读"],
  "personality": {
    "性格特点": "积极向上，好学进取",
    "情绪倾向": "积极乐观",
    "社交风格": "友好开放"
  },
  "communication_style": {
    "表达方式": "直接真诚",
    "语言风格": "专业而友好",
    "回复习惯": "及时回应"
  }
}

❓ 问题: 我想提升自己的技术能力，你有什么建议吗？
🤖 个性化回答: 基于你对AI和深度学习的浓厚兴趣，建议你...
```

## 🔧 API接口

### 1. 创建用户
```python
result = await create_user("用户名", "手机号")
```

### 2. 导入聊天记录
```python
messages = [
    {
        "message_type": "sent",
        "content": "消息内容",
        "timestamp": "2024-01-15 10:30:00",
        "contact_name": "联系人"
    }
]
result = await import_wechat_messages_llm(user_id, json.dumps(messages))
```

### 3. LLM用户画像分析
```python
result = await analyze_user_llm(user_id)
```

### 4. 个性化问答
```python
result = await personalized_qa_llm(user_id, "你的问题")
```

## 🎯 核心功能详解

### 用户画像分析
LLM会分析聊天记录并生成包含以下维度的用户画像：
- **兴趣爱好**: 基于聊天内容识别用户兴趣点
- **性格特征**: 分析用户的性格倾向和行为模式  
- **沟通风格**: 识别用户的表达习惯和语言特点
- **价值观念**: 提取用户的价值观和人生态度
- **行为模式**: 总结用户的典型行为特征

### RAG检索增强
- **语义检索**: 基于问题内容检索相关的历史对话
- **相似度匹配**: 使用向量相似度找到最相关的记忆片段
- **上下文整合**: 将检索结果与当前问题结合生成回答

### 个性化Prompt
- **画像融入**: 将用户画像信息嵌入到系统提示词中
- **风格模拟**: 让AI助手采用符合用户特点的表达方式
- **记忆利用**: 参考相关的历史对话提供个性化建议

## 🔒 隐私保护

- **本地部署**: 大模型和向量数据库都在本地运行
- **数据不出域**: 用户聊天记录不会上传到云端
- **加密存储**: 敏感数据采用加密存储方式
- **访问控制**: 严格的用户权限和数据访问控制

## 🚀 性能优化

- **模型量化**: 支持4bit/8bit量化减少内存占用
- **向量索引**: 使用FAISS进行高效的向量检索
- **缓存机制**: 缓存用户画像和常用查询结果
- **异步处理**: 全异步架构提升并发性能

## 🔮 扩展方向

### 短期优化
- 支持更多开源LLM模型（Llama、ChatGLM等）
- 添加多模态支持（图片、语音分析）
- 优化向量检索算法和索引策略

### 中期发展  
- 集成知识图谱增强用户理解
- 支持群聊分析和社交网络建模
- 添加情感变化趋势分析

### 长期愿景
- 构建完整的数字人格系统
- 支持跨平台数据整合
- 提供开放API生态

## 📊 技术对比

| 特性 | 基础版本 | LLM版本 |
|------|----------|---------|
| 用户画像 | 关键词匹配 | LLM深度分析 |
| 回答质量 | 模板化 | 自然对话 |
| 个性化程度 | 规则驱动 | 画像驱动 |
| 扩展性 | 有限 | 高度可扩展 |
| 智能程度 | 中等 | 高 |

## 🤝 贡献指南

欢迎提交Issue和Pull Request！

### 开发环境设置
```bash
git clone <repo>
cd mcp_central/digital_twin
pip install -r requirements_llm.txt
ollama pull qwen2.5:7b
python3 test_llm_standalone.py
```

### 代码规范
- 遵循PEP 8编码规范
- 添加必要的类型注解
- 完善的错误处理和日志记录
- 编写单元测试

## 📄 许可证

本项目遵循项目根目录的LICENSE文件。
